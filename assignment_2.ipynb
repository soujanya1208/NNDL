{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment 2",
      "provenance": [],
      "authorship_tag": "ABX9TyO9AlVNKoInLiq1+F8kvVQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soujanya1208/NNDL/blob/main/assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EnNmilRK2Nnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59fdadd6-802f-4804-d383-e4c73a6bd65e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i) Word2vec\\n    ii) USE\\niii)ELMO\\niv) GP2\\nv) Sentence-BERT'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 1.Convert the above paragraph into vectors using:<br>\n",
        "'''i) Word2vec\n",
        "    ii) USE\n",
        "iii)ELMO\n",
        "iv) GP2\n",
        "v) Sentence-BERT'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_string='''paragraph is a series of sentences that are organized and coherent, and are all  related  to  a  single  topic.  Almost  every  piece  of  writing  you  do  that  is \n",
        "longer  than  a  few  sentences  should  be  organized  into  paragraphs.  This  is because paragraphs show a reader where the subdivisions of an essay begin \n",
        "and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "\n",
        "Paragraphs  can  contain  many  different  kinds  of  information.  A  paragraph could  contain  a  series  of  brief  examples  or  a  single  long  illustration  of  a \n",
        "general  point.  It  might  describe  a  place like kolkata,  character,  or  process;  narrate  a series of events; compare or contrast two or more things; classify items into \n",
        "categories;  or  describe  causes  and  effects.  Regardless  of  the  kind  of information they contain, all paragraphs share certain characteristics. One \n",
        "of the most important of these is a topic sentence.'''"
      ],
      "metadata": {
        "id": "tKBQN33o2m67"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import corpora,models,similarities\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "UpUC7Du46ICA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code to convert paragraph to sentences\n",
        "def essay_to_sentences(paragraph):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(paragraph.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    return sentences\n",
        "\n",
        "sentences=essay_to_sentences(my_string)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "id": "iZnTD3-u6NTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordvecs=[nltk.word_tokenize(sent) for sent in sentences]\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops=list(set(stopwords.words(\"english\")))\n",
        "for i in wordvecs:\n",
        "  for j in i:\n",
        "    if j in stops:\n",
        "      i.remove(j)\n",
        "    elif len(j)==1:\n",
        "      i.remove(j)\n",
        "\n",
        "model=gensim.models.Word2Vec(wordvecs,min_count=1,size=32)"
      ],
      "metadata": {
        "id": "konGM9tC24P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing vector form of word 'paragraph'\n",
        "model['paragraph']"
      ],
      "metadata": {
        "id": "rV_iuTr925D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding similar words of word 'sentence' in given paragraph\n",
        "model.most_similar('sentence')"
      ],
      "metadata": {
        "id": "m27C2_Yj25Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "use= hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "#converting to vectors\n",
        "embeddings=use(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "yOBindne3NTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape= \",embeddings[0].shape)\n",
        "#each sentence is converted into vector having 512 values\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(embeddings[0]))"
      ],
      "metadata": {
        "id": "0oFNiMg-3QM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "embeddings2=bert(sentences)\n",
        "print(embeddings2)"
      ],
      "metadata": {
        "id": "H-xEl-8K3T94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",embeddings2[0].shape)\n",
        "#each sentence is converted into vector having 128 values\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(embeddings2[0]))"
      ],
      "metadata": {
        "id": "cGYi0XFC3X3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow>=2.0.0\"\n",
        "!pip install --upgrade tensorflow-hub"
      ],
      "metadata": {
        "id": "7UQdmrjt3baq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "#1024 sized vectors\n",
        "elmo=hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)\n",
        "embeddings=elmo(\n",
        "    sentences,\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "init=tf.initialize_all_variables()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "print(\"\\n\\n\")\n",
        "print(sess.run(embeddings[0]))\n",
        "print(\"shape=\",embeddings[0].shape)"
      ],
      "metadata": {
        "id": "-A45QyIF3feE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "r95LDEa03i06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptokenizer=transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model=transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "output=gptokenizer.encode(my_string,add_special_tokens=False,return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "DwgtoMEZ3vMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",output.shape)\n",
        "output"
      ],
      "metadata": {
        "id": "J7rx4qol3wLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find named entities (NER) for the above paragraph?"
      ],
      "metadata": {
        "id": "neX5Dtv74CxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "ner=spacy.load('en')\n",
        "result=ner(my_string)\n",
        "\n",
        "for word in result.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "id": "pCBhJasr3_Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "id": "GstVzi-v4HYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(result,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "h5P31HhG4LJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding named entities of another paragraph\n",
        "resultss=ner(\"Cricket was introduced to India by British sailors in the 18th century, and the first cricket club was established in 1792. India's national cricket team did not play its first Test match until 25 June 1932 at Lord's, becoming the sixth team to be granted test cricket status. From 1932 India had to wait until 1952, almost 20 years for its first Test victory. In its first fifty years of international cricket, India didn't gain much success, winning only 35 of the first 196 Test matches it played. The team, however, gained strength in the 1970s with the emergence of players like Gavaskar, Viswanath, Kapil Dev, and the Indian spin quartet.\")\n",
        "for word in resultss.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "id": "lA4ZhiL65FNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(resultss,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "ptaRMUTJ5I4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Find similar sentences(repeated sentences) from the above paragraph?\n",
        "pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "Xbo8ut-j5MM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se_embeddings = sbert_model.encode(sentences)\n",
        "q1_vec= sbert_model.encode(sentences[0])\n",
        "\n",
        "#cosine similarity function\n",
        "#identifies similarity between 2 sentences\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "for sent in sentences:\n",
        "  sim = cosine(q1_vec, sbert_model.encode([sent])[0])\n",
        "  #if similarity ==1 => repeated sentence\n",
        "  #if similarity > 0.6 => similar sentence\n",
        "  if sim>0.6:\n",
        "    print(\"Sentence1 =\",sentences[0],\"\\n \\nSentence2=\", sent, \"\\n\\nsimilarity = \", sim,end=\"\\n ----------------------------- \\n\")"
      ],
      "metadata": {
        "id": "JhZcPNkb5PiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Explain POS tagging with HMM?"
      ],
      "metadata": {
        "id": "10n7H3Fs5P2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}